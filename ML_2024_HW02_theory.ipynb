{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71e5a95",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681ded3",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but *discouraged*.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cee62",
   "metadata": {},
   "source": [
    "## Task 1: Kernel theory [8 points]\n",
    "\n",
    "Let $K(x, x'): \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$ be a kernel, and $\\phi: \\mathbb{R}^n \\to \\mathbb{R}^m$ its **unknown** feature mapping. For $x, x' \\in \\mathbb{R}^n$ derive the squared Euclidean distance between $\\phi(x)$ and $\\phi(x')$ only in terms of $K(\\cdot, \\cdot)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e07a2f",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15374dff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "256f6720",
   "metadata": {},
   "source": [
    "## Task 2: SVM [9 points]\n",
    "\n",
    "Show that for a two-class SVM classifier trained on a linearly separable dataset $(x_i, y_i)_{i =1}^n$ the following upper bound on the leave-one-out-cross-validation error holds true:\n",
    "\n",
    "$$\n",
    "L_1OCV = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne f_i(x_i)) \\le \\frac{|SV|}{n},\n",
    "$$\n",
    "\n",
    "where $\\delta(c) = 1$ if $c$ is True and $\\delta(c) = 0$ if $c$ is False;  \n",
    "for all $i = 1, \\dots, n$ $f_i(x_i)$ is the SVM classifier fitted on the entire data without the observation $(x_i, y_i)$ and $|SV|$ is the number of support vectors of the SVM classifier fitted on the entire data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3f490",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff523e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ddb203f",
   "metadata": {},
   "source": [
    "## Task 3. Decision Tree Leaves [6 points]\n",
    "\n",
    "Consider a leaf of a decision tree that consists of object-label pairs $(x_{1}, y_{1}), \\dots, (x_{n}, y_{n})$.\n",
    "\n",
    "The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefc354",
   "metadata": {},
   "source": [
    "Find the **optimal prediction** in the leaf, for a regression tree, i.e. $y_{i} \\in \\mathbb{R}$, and squared percentage error loss $\\mathcal{L}(y, \\hat{y}) = \\cfrac{\\left(y - \\hat{y} \\right)^{2}}{y^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa5b6e",
   "metadata": {},
   "source": [
    "### Your solution:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
