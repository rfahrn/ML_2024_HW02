{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bb8c75",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fdd20",
   "metadata": {},
   "source": [
    "To solve this task efficiently, here are some practical suggestions:\n",
    "\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [sklearn](https://scikit-learn.org/stable/). Also remember to use tutorials, lecture slides, and other resources.\n",
    "\n",
    "\n",
    "* Instead of rewriting existing code try to use **BUILT-IN METHODS** available in the libraries.\n",
    "\n",
    "\n",
    "* To complete this part of the homework, you have to write some **CODE** directly inside the specified places in the notebook **CELLS**.\n",
    "\n",
    "\n",
    "* In some problems you are asked to provide a short discussion of the results. In these cases you have to create a **MARKDOWN** cell with your comments right after the corresponding code cell.\n",
    "\n",
    "\n",
    "* For every separate problem, you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if a reviewer executes your code, the output will be the same (with all the corresponding plots) as in your uploaded notebook. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudorandomness.\n",
    "\n",
    "\n",
    "* Your code must be readable. For this purpose, try to include **necessary** (and not more) comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY**.\n",
    "\n",
    "\n",
    "* Many `sklearn` algorithms support multithreading (Ensemble Methods, Cross-Validation, etc.). Check if the particular algorithm has `n_jobs` parameter and set it to `-1` to use all the cores.\n",
    "\n",
    "\n",
    "* In the end you need to hand in a **single zip file** containing **two notebooks** (theory and practice) as well as the **html exported version** of this notebook (**4 files in total**).\n",
    "\n",
    "\n",
    "To begin let's import the essential (for this assignment) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set default parameters for plots\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ec49b",
   "metadata": {},
   "source": [
    "## Part 1: Linear SVM (face detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4d483",
   "metadata": {},
   "source": [
    "In this part you will need to implement a linear SVM classifier via the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca8d89",
   "metadata": {},
   "source": [
    "First, let us have a look at the data we want to train our algorithm on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16251d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the faces data\n",
    "data = loadmat('faces.mat')\n",
    "labels = np.squeeze(data['Labels'])\n",
    "data = data['Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811f220",
   "metadata": {},
   "source": [
    "Let us visualize some examples from the dataset. The dataset contains a set of $24 \\times 24$ grayscale images labeled as face/non-face. Our goal is to train a classifier on this data. For calculations we will treat the samples as flattened vectors, the same way as they are stored. For visualization one needs to reshape the samples to $24 \\times 24$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bed875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "samples_per_class = 10\n",
    "classes = [-1, 1]\n",
    "train_imgs = np.reshape(data, [-1, 24, 24], order='F')\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.equal(labels, cls))\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = y * samples_per_class + i + 1\n",
    "        plt.subplot(len(classes), samples_per_class, plt_idx)\n",
    "        plt.imshow(train_imgs[idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de28494",
   "metadata": {},
   "source": [
    "## Task 1.1: SVM loss function [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53543c94",
   "metadata": {},
   "source": [
    "In the lectures we have seen the SMO algorithm, which works in the dual domain. In this assignment we explore **Pegasos**, an optimization algorithm in the primal domain.\n",
    "\n",
    "Recall the formulation of the SVM optimization problem as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\min_{w, b}\n",
    "& & \\frac{1}{2}||w||^2 + C\\sum_{i=1}^m \\xi_i \\\\\n",
    "& \\ \\text{ s.t.}\n",
    "& & y^{(i)}(w^Tx^{(i)}+b) \\geq 1-\\xi_i, \\; i = 1, \\ldots, m \\\\\n",
    "& & & \\xi_i \\geq 0, \\; i = 1, \\ldots, m \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Let $f(x)=w^Tx+b$. The constraints can then be written as $y^{(i)}f(x^{(i)})\\geq 1-\\xi_i$. Together with the constraints $\\xi_i \\geq 0$ this leads to $\\xi_i=\\max(0, 1-y^{(i)}f(x^{(i)}))$. The above constraint optimization problem is therefore equivalent to the following **unconstrained** problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{w, b} \\frac{\\lambda}{2}||w||^2 + \\frac{1}{m}\\sum_{i=1}^m \\max(0, 1-y^{(i)}f(x^{(i)}))\n",
    "\\end{equation}\n",
    "\n",
    "The first term in this objective is a regularization term (prevents overfitting) and the second term measures the classification loss. Here the parameter $\\lambda=1/C$ is a **hyper-parameter** that controls the relative weight of  both losses.\n",
    "\n",
    "\n",
    "**TODO**: Implement the **unconstrained** objective function for SVM in the following *svm_loss* function according to the specifications.\n",
    "\n",
    "***HINT***: Write a small test for the provided values of w, b and C. Make sure your function passes the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(w, b, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the loss of a linear SVM w.r.t. the given data and parameters\n",
    "\n",
    "    Args:\n",
    "        w: Parameters of shape [num_features]\n",
    "        b: Bias (a scalar)\n",
    "        X: Data matrix of shape [num_data, num_features]\n",
    "        y: Labels corresponding to X of size [num_data, 1]\n",
    "        lambda_: Regularization hyper-parameter\n",
    "\n",
    "    Returns:\n",
    "        l: The value of the objective for a linear SVM\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #######################################################################\n",
    "    # TODO:                                                               #\n",
    "    # Compute and return the value of the unconstrained SVM objective     #\n",
    "    #                                                                     #\n",
    "    #######################################################################\n",
    "    \n",
    "    l = None\n",
    "\n",
    "    #######################################################################\n",
    "    #                         END OF YOUR CODE                            #\n",
    "    #######################################################################\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your cost-function\n",
    "w_0 = np.zeros(data.shape[1])\n",
    "b_0 = 0.\n",
    "l_0 = svm_loss(w_0, b_0, data, labels, 1.)\n",
    "\n",
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# What should be the expected output of svm_loss with the given params#\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "expected_output = None\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################\n",
    "\n",
    "assert np.allclose(l_0, expected_output), \"Something is wrong\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed8e4d",
   "metadata": {},
   "source": [
    "## Task 1.2: SVM gradient [9 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a527f4d",
   "metadata": {},
   "source": [
    "**TODO**: Implement the gradient of the above unconstrained objective w.r.t. to the parameters $w$ and $b$. The gradient will be computed on a mini-batch (i.e., a random subset of the training set).\n",
    "\n",
    "**Hint**: Don't worry about the fact that $\\max(0, 1-y^{(i)}f(x^{(i)}))$ is not differentiable at $1-y^{(i)}f(x^{(i)})=0$. Just pick a one-sided gradient (this is called a subgradient for convex functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_gradient(w, b, x, y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute gradient for SVM w.r.t. to the parameters w and b on a mini-batch (x, y)\n",
    "\n",
    "    Args:\n",
    "        w: Parameters of shape [num_features]\n",
    "        b: Bias (a scalar)\n",
    "        x: A mini-batch of training example [k, num_features]\n",
    "        y: Labels corresponding to x of size [k]\n",
    "        lambda_: Regularization hyper-parameter\n",
    "\n",
    "    Returns:\n",
    "        grad_w: The gradient of the SVM objective w.r.t. w of shape [num_features]\n",
    "        grad_v: The gradient of the SVM objective w.r.t. b of shape [1]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # TODO:                                                               #\n",
    "    # Compute the gradient for a particular choice of w and b.            #\n",
    "    # Compute the partial derivatives and set grad_w and grad_b to the    #\n",
    "    # partial derivatives of the cost w.r.t. both parameters in theta     #\n",
    "    #                                                                     #\n",
    "    #######################################################################\n",
    "\n",
    "    grad_w = None\n",
    "    grad_b = None\n",
    "\n",
    "    #######################################################################\n",
    "    #                         END OF YOUR CODE                            #\n",
    "    #######################################################################\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc006654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "x_ = np.ones([2, 10])\n",
    "y_ = np.array([1, -1])\n",
    "w_0 = np.zeros(10)\n",
    "b_0 = 0.\n",
    "grad_w, grad_b = svm_gradient(w_0, b_0, x_, y_, 1.)\n",
    "\n",
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# What should be the expected output of svm_gradient                  #\n",
    "# with the given params                                               #\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "expected_grad_w = None\n",
    "expected_grad_b = None\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################\n",
    "\n",
    "assert np.allclose(grad_w, expected_grad_w), \"Something is wrong\"\n",
    "assert np.allclose(grad_b, expected_grad_b), \"Something is wrong\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1d992",
   "metadata": {},
   "source": [
    "## Task 1.3: SVM Solver [18 points]\n",
    "\n",
    "You will implement the **Pegasos** algorithm - a variant of SGD - to solve for the parameters $w$ and $b$. \n",
    "\n",
    "The algorithm was introduced in the following [Paper](http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf) (see Figure 2). It is essentially Stochastic Gradient Descent on mini-batches + a specific choice for the learning rate giving convergence guarantees. The required steps are outlined in the following class implementation. For more details, please refer to the [Paper](http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf).\n",
    "\n",
    "**TODO**: Implement the Pegasos algorithm according to specs. Tune the hyper-parameter **C** to get at least 90% accuracy on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM:\n",
    "    def __init__(self, lambda_):\n",
    "        self.lambda_ = lambda_\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def fit(self, X, y, num_iter=30000, num_per_batch=100, verbose=False):\n",
    "        \"\"\"\n",
    "        Pegasos SVM solver.\n",
    "\n",
    "        Args:\n",
    "            X: Data matrix of shape [num_train, num_features]\n",
    "            y: Labels corresponding to X of size [num_train]\n",
    "            num_iter: Number of iterations\n",
    "            num_per_batch: Number of samples per mini-batch\n",
    "\n",
    "        Returns:\n",
    "            theta: The value of the parameters after training\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.b = 0.\n",
    "        for t in range(num_iter):\n",
    "            start = time.time()\n",
    "            \n",
    "            #######################################################################\n",
    "            # TODO:                                                               #\n",
    "            # Perform one step of stochastic gradient descent:                    #\n",
    "            #   - Select a single training batch at random                        #\n",
    "            #   - Update theta based on alpha and using gradient_function         #\n",
    "            #                                                                     #\n",
    "            #######################################################################\n",
    "            \n",
    "            # 1st Step: Sample a random mini-batch of size num_per_batch\n",
    "\n",
    "            # 2nd Step: Compute the learning-rate n_t=1/(lambda*t)\n",
    "\n",
    "            # 3rd Step: Compute the gradients and update the parameters as\n",
    "\n",
    "            #######################################################################\n",
    "            #                         END OF YOUR CODE                            #\n",
    "            #######################################################################\n",
    "            \n",
    "            if verbose and t % 5000 == 0:\n",
    "                exec_time = time.time() - start\n",
    "                loss = svm_loss(self.w, self.b, X, y, self.lambda_)\n",
    "                print(f'Iter {t}/{num_iter}: cost = {loss}  ({exec_time}s)')\n",
    "\n",
    "    def predict_score(self, X):\n",
    "        \"\"\"\n",
    "        Predicts a score for each sample (how confident the prediction is)\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape [num_train, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Predictions of shape [num_train] - real numbers\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Write a function that predicts the \"score\" (condifence) for each    #\n",
    "        # sample:                                                             #\n",
    "        #                                                                     #\n",
    "        #######################################################################\n",
    "        \n",
    "        preds = None\n",
    "\n",
    "        #######################################################################\n",
    "        #                         END OF YOUR CODE                            #\n",
    "        #######################################################################\n",
    "\n",
    "        return preds                \n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels on X.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape [num_train, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Predictions of shape [num_train]\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Write a function that predicts the class label {-1, 1} given the    #\n",
    "        # data matrix X and trained params w and b. Use predict_score method  # \n",
    "        #                                                                     #\n",
    "        #######################################################################\n",
    "\n",
    "        preds = None\n",
    "\n",
    "        #######################################################################\n",
    "        #                         END OF YOUR CODE                            #\n",
    "        #######################################################################\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates accuracy of the model on X.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape [num_train, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Model's accuracy score.\n",
    "        \"\"\"\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Write a functions that calculates the performance of the model on X #\n",
    "        # in terms of accuracy                                                #\n",
    "        #                                                                     #\n",
    "        #######################################################################\n",
    "        \n",
    "        score = None\n",
    "\n",
    "        #######################################################################\n",
    "        #                         END OF YOUR CODE                            #\n",
    "        #######################################################################\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f88b7",
   "metadata": {},
   "source": [
    "Now we are going to train the model. First let's split the data into train and test sets and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616843b9-0c75-4853-acb3-b3fba8a2490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9d907",
   "metadata": {},
   "source": [
    "Now let us create a classifier instance and **fit** in on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79816e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will measure the execution time\n",
    "start = time.time()\n",
    "\n",
    "C = 0.01\n",
    "lambda_ = 1/C\n",
    "my_svm = LinearSVM(lambda_=lambda_)\n",
    "my_svm.fit(X_train, y_train, num_iter=30000, num_per_batch=64, verbose=True)\n",
    "\n",
    "exec_time = time.time()-start\n",
    "print('Total exection time: {}s'.format(exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db937ef0",
   "metadata": {},
   "source": [
    " We can have a look at what theta has learned to recognise as \"face\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(my_svm.w, [24, 24], order='F'))\n",
    "plt.title('Learned w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762f427",
   "metadata": {},
   "source": [
    "Make the predictions, calculate the accuracy. Tune the hyperparameter C to get at least **90%** accuracy on the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found C = {}, lambda = {}'.format(C, lambda_))\n",
    "print('Accuracy train: {}'.format(my_svm.score(X_train, y_train)))\n",
    "print('Accuracy test: {}'.format(my_svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9688ad-6019-4f3e-a182-53b9531899c5",
   "metadata": {},
   "source": [
    "## Part 2: Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469eb0a6-0776-4ce6-a1d6-3306b4279447",
   "metadata": {},
   "source": [
    "## Task 2.1: ONE-VS-ALL SVM [12 points]\n",
    "\n",
    "SVM is designed to solve a binary classification problem. What if we want to solve _multiclass classification_, meaning that we want to classify sample in 1 to K labels (with K>2)?\n",
    "\n",
    "We can still use binary SVM to solve this problem in a _one-vs-all_ manner, meaning that we can train _K_ binary classifiers distinguishing class $k$ vs the rest of the classes.\n",
    "\n",
    "You will implement multiclass classification using your LinearSVM implementation as follows:\n",
    "1. Initialize MulticlassLinearSVM with _K_ LinearSVMs\n",
    "2. For each _k=1..K_ labels:  \n",
    "    a. Convert multiclass labels into binary labels: _y==1_ if _target==k_, _y==-1_ if _target!=k_  \n",
    "    b. Train a binary classifier with this labels\n",
    "3. To predict a label for an unknown sample:  \n",
    "    a. Get a prediction score for each binary classifier  \n",
    "    b. Return the label for which the score is the highest\n",
    "\n",
    "**TODO**: Implement the multiclass SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b8355-a93b-405e-b9c9-b314c7ed281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLinearSVM:\n",
    "    def __init__(self, lambda_, K_classes):\n",
    "        self.lambda_ = lambda_\n",
    "        self.K_classes = K_classes\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Initialize the class with K_classes instances of LinearSVM          #\n",
    "        #######################################################################\n",
    "        self.classifiers = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, num_iter=30000, num_per_batch=100, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit K classifiers\n",
    "\n",
    "        Args:\n",
    "            X: Data matrix of shape [num_train, num_features]\n",
    "            y: Labels corresponding to X of size [num_train]; values from 0..K_classes-1\n",
    "            num_iter: Number of iterations\n",
    "            num_per_batch: Number of samples per mini-batch\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Train K_classes binary classifiers                                  #\n",
    "        #   - Convert integer labels 0..K-1 to binary labels 1 / -1 for a given #\n",
    "        #     binary classifier                                               #\n",
    "        #   - Train the current binary classifier                             #\n",
    "        #                                                                     #\n",
    "        #######################################################################\n",
    "        \n",
    "        for k in range(0, self.K_classes):\n",
    "            start = time.time()\n",
    "            \n",
    "            current_classifier = self.classifiers[k]\n",
    "            \n",
    "            #######################################################################\n",
    "            # TODO:                                                               #\n",
    "            # Convert multiclass labels y to binary labels classifying            #\n",
    "            # y_bin = 1  if y==k vs y!=k                                          #\n",
    "            # y_bin = -1 if y!=k vs y!=k                                          #\n",
    "            #######################################################################\n",
    "\n",
    "\n",
    "            \n",
    "            #######################################################################\n",
    "            # TODO:                                                               #\n",
    "            # Train the binary classifier with the converted labels               #\n",
    "            # Don't forget to use this method's arguments                         #\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            #######################################################################\n",
    "            #                         END OF YOUR CODE                            #\n",
    "            #######################################################################\n",
    "            \n",
    "            exec_time = time.time() - start\n",
    "            print(f'Classifier {k+1}/{self.K_classes}: time ({exec_time}s)')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels on X.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape [num_train, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Predictions of shape [num_train] in {0..K_classes-1}\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Write a function that predicts the class label {0, .., K_classes-1} #\n",
    "        # Predict the score for every binary classifier (use predict_score)   #\n",
    "        # Return the label for which the classifier gives the greatest score  #\n",
    "        #                                                                     #\n",
    "        #######################################################################       \n",
    "        \n",
    "\n",
    "        #######################################################################\n",
    "        #                         END OF YOUR CODE                            #\n",
    "        #######################################################################\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates accuracy of the model on X.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape [num_train, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Model's accuracy score.\n",
    "        \"\"\"\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Write a functions that calculates the performance of the model on X #\n",
    "        # in terms of accuracy                                                #\n",
    "        #                                                                     #\n",
    "        #######################################################################\n",
    "        \n",
    "\n",
    "        #######################################################################\n",
    "        #                         END OF YOUR CODE                            #\n",
    "        #######################################################################\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615e4a1-c0bf-4af3-8b92-217f9cee41db",
   "metadata": {},
   "source": [
    "### Simple (non-exhaustive) test\n",
    "The code below should execute without errors and give ~25% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1384cb3-d815-409d-8833-fe951c3fc99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10.0\n",
    "lambda_ = 1/C\n",
    "\n",
    "n_classes = 4\n",
    "n_samples = 2000\n",
    "data_random = np.random.randn(n_samples, 2)\n",
    "labels_random = np.array([np.random.randint(n_classes) for i in range(n_samples)])\n",
    "\n",
    "mc_svm = MulticlassLinearSVM(lambda_=lambda_, K_classes=n_classes)\n",
    "mc_svm.fit(data_random, labels_random, num_iter=1000, num_per_batch=64, verbose=True)\n",
    "\n",
    "pred = mc_svm.predict(data_random)\n",
    "print('Accuracy: ', mc_svm.score(data_random, labels_random))\n",
    "\n",
    "exec_time = time.time()-start\n",
    "print('Total execution time: {}s'.format(exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3f3ba-648a-4294-9c30-2c0c140440e5",
   "metadata": {},
   "source": [
    "## Classification on FashionMNIST dataset\n",
    "We'll train a multiclass SVM on FashionMNIST dataset, containing $28\\times28$ grayscale images of 10 categories of clothing.  \n",
    "Let's load and visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64aca6a-b0a3-40e6-9c74-6ae403154207",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_data = np.load('fashion_mnist.npz')\n",
    "X_train, X_test, y_train, y_test = fmnist_data['x_train'], fmnist_data['x_test'], fmnist_data['y_train'], fmnist_data['y_test']\n",
    "\n",
    "# Visualize some examples from the dataset.\n",
    "samples_per_class = 10\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "train_imgs = np.reshape(X_train, [-1, 28, 28], order='F')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.equal(y_train, cls))\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = y * samples_per_class + i + 1\n",
    "        plt.subplot(len(classes), samples_per_class, plt_idx)\n",
    "        plt.imshow(train_imgs[idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744fc46-3dc9-449c-a44b-be9d31fe0f7e",
   "metadata": {},
   "source": [
    "Now let's normalize the data, train the model and visualize the weights for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a94ef4-2feb-45e4-885e-9a4902f03e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e55558-6593-4fa3-93c6-61fab2aa0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.01\n",
    "lambda_ = 1./C\n",
    "my_svm = MulticlassLinearSVM(lambda_=lambda_, K_classes=10)\n",
    "\n",
    "my_svm.fit(X_train_scaled, y_train, num_iter=10000, num_per_batch=64, verbose=True)\n",
    "\n",
    "exec_time = time.time()-start\n",
    "print('Total exection time: {}s'.format(exec_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c5c04-41d9-4c60-9d6f-1370b0ce8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('C = {}, lambda = {}'.format(C, lambda_))\n",
    "print('Accuracy train: {}'.format(my_svm.score(X_train_scaled, y_train)))\n",
    "print('Accuracy test: {}'.format(my_svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51515743-9453-4e31-9a86-51054822e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in my_svm.classifiers:\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(np.reshape(classifier.w, [28, 28], order='F'))\n",
    "    plt.title('Learned w')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e29079-fb4f-4e5d-9195-181d88d0b149",
   "metadata": {},
   "source": [
    "**TODO:** Tune the parameter **C** to get at least **80%** test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ecc80-7fc0-4abc-885b-b277f7affa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = \n",
    "lambda_ = 1./C\n",
    "my_svm = MulticlassLinearSVM(lambda_=lambda_, K_classes=10)\n",
    "\n",
    "my_svm.fit(X_train_scaled, y_train, num_iter=10000, num_per_batch=64, verbose=True)\n",
    "\n",
    "exec_time = time.time()-start\n",
    "print('Total exection time: {}s'.format(exec_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e2fc4-2c97-4232-add7-5218a26d0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found C = {}, lambda = {}'.format(C, lambda_))\n",
    "print('Accuracy train: {}'.format(my_svm.score(X_train_scaled, y_train)))\n",
    "print('Accuracy test: {}'.format(my_svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540eed4",
   "metadata": {},
   "source": [
    "## Part 3: SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa834f",
   "metadata": {},
   "source": [
    "## Task 3.1: Compare RBF and linear kernels [10 points]\n",
    "\n",
    "**TODO:** Compare SVM with linear and RBF kernels on toy datasets. Implement *plot_classifiers_predictions* function. Make sure to plot the decision regions as in the previous assignment as well as the support vectors.\n",
    "\n",
    "**HINT:** You may use ideas from this [webpage](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae889ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifiers_predictions(X, y, classifiers):\n",
    "    \"\"\"\n",
    "    Plots the decision regions and support vectors of the classifiers\n",
    "    fit on X and y.\n",
    "\n",
    "    Args:\n",
    "        X: Data matrix of shape [num_train, num_features]\n",
    "        y: Labels of shape [num_train]\n",
    "        classifiers: A list of classfifier objects\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=len(classifiers), nrows=1, figsize=(16, 8))\n",
    "\n",
    "    for classifier, axis in zip(classifiers, axes.flat):\n",
    "        #######################################################################\n",
    "        # TODO:                                                               #\n",
    "        # Implement the plotting function                                     # \n",
    "        #                                                                     #\n",
    "        #######################################################################\n",
    "\n",
    "        # Fit the classifier to the data\n",
    "\n",
    "        # Plot the decision regions\n",
    "\n",
    "        # Plot the support vectors\n",
    "\n",
    "        #######################################################################\n",
    "        #                         END OF YOUR CODE                            #\n",
    "        #######################################################################\n",
    "        \n",
    "        axis.set_title('{}, accuracy = {:.2f}'.format(\n",
    "            classifier.__class__.__name__, classifier.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efefe6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "linear_svc = SVC(kernel=\"linear\")\n",
    "rbf_svc = SVC(kernel=\"rbf\")\n",
    "\n",
    "classifiers = [linear_svc, rbf_svc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae825ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blobs, y_blobs = make_blobs(n_samples=300, centers=[[-1.5, -1.5], [1.5, 1.5]])\n",
    "plot_classifiers_predictions(X_blobs, y_blobs, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd62635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=0)\n",
    "plot_classifiers_predictions(X_moons, y_moons, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.1, random_state=0, factor=0.6)\n",
    "plot_classifiers_predictions(X_circles, y_circles, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3327e",
   "metadata": {},
   "source": [
    "## Task 3.2: Questions [6 points]\n",
    "\n",
    "* Play with the hyperparameters. How do the hyper-parameters influence the classifier? What happens for extreme values of the hyper-parameters?\n",
    "\n",
    "***Your Answer:***\n",
    "\n",
    "* Linear SVM vs. Gaussian Kernel SVM: Give advantages and disadvantages of both approaches. \n",
    "\n",
    "***Your Answer:***\n",
    "\n",
    "* Linear SVM vs. Gaussian Kernel SVM: In what setting would you pick one method over the other? Answer in terms of number of training examples $m_{train}$ and feature dimension $d$\n",
    "\n",
    "***Your Answer:***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8aa59-b789-4d73-a1f0-3b8ca2a373a5",
   "metadata": {},
   "source": [
    "## Part 4: Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483ed60-f381-44a0-abe2-111fbbe9593f",
   "metadata": {},
   "source": [
    "In this part you will be working on a text classification task. The IMDB dataset contains movie reviews labeled with positive/negative scores. Your goal here is to train a model to classify these texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec05c2-f056-47b5-82e9-44c5cb40ba82",
   "metadata": {},
   "source": [
    "Download the data from the [competition page](https://www.kaggle.com/t/a075874d324e402892eb5b4a31755fbb) and put it to your working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf91d41-4e24-4d15-a126-4572f22f3021",
   "metadata": {},
   "source": [
    "We will use the *pandas* module to read, store and work with the data. The *pandas* module is a powerful library that provides us with a lot of useful functions and data structures that are designed to work with tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7cb83-cdab-4fa9-96b0-e7a67acb2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the data\n",
    "imdb_dataset_train = pd.read_csv(\"train_imdb_dataset.csv\", index_col=0)\n",
    "reviews_test = pd.read_csv(\"test_reviews.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbf588-5c4c-4ad8-88e8-6cdea7f81ba2",
   "metadata": {},
   "source": [
    "In *pandas* the main data structure is the *pd.DataFrame*. It represents the data in a form of a table. You can access the rows using the indices and the columns with the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70aae0-cfee-479d-a80e-c6161ff9a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59aa79-b7e4-462b-bb36-4d7c90c9ffee",
   "metadata": {},
   "source": [
    "Let us have a look at the first row of the training *DataFrame*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028caed-e448-4953-b253-26591936f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"Sentiment:\", imdb_dataset_train[\"sentiment\"][i])\n",
    "print(\"Review:\", imdb_dataset_train[\"review\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39678e-47c5-4fb2-9816-84c68383233c",
   "metadata": {},
   "source": [
    "## Task 4.1: Preprocessing [3 points]\n",
    "\n",
    "One needs to transform the data to the format that can be used with the known classifiers. First, let us map sentiments to the categorical labels.\n",
    "\n",
    "**TODO:** Transform the *sentiments* to *labels*, an array containing 0s and 1s. You can use *OrdinalEncoder* from *sklearn.preprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a145ffa-79c3-4f24-bacd-9ec0f6c5416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# Transform sentiments to labels.                                     # \n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "y_train = None\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c05bf6-a740-44aa-841d-1c83b14202c3",
   "metadata": {},
   "source": [
    "Then we need to represent each text as a bag of words.\n",
    "\n",
    "Using *CountVectorizer* from *sklearn.feature_extraction.text* we can transform the *reviews* to a data matrix *X* of shape [num_reviews, vocabulary_size], where each row represents a single text and each column indicates the number of occurences of a specific word across the dataset.\n",
    "Notice that the Vectorizer has a lot of useful arguments. These could potentially influence the performance of the models. Make sure to at least set *stop_words=\"english\"*. This will make the Vectorizer ignore frequent words, that are useless for classification (e.g. articles, prepositions... ). Make sure you fit the Vectorizer to the union of the training and test reviews, otherwise you might miss some of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06a401-9041-4ce0-8abc-d5bc95a6e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# Transform reviews to the words-frequency matrices                   #\n",
    "# X_train and X_test                                                  #\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "X_train = None\n",
    "X_test = None\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817168b7-c6fc-418c-afb9-1493ed25c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the shape of the data matrix\n",
    "assert X_train.shape[1] == X_test.shape[1], \"Train and test matrices should have the same number of tokens\"\n",
    "print(\"Number of tokens found:\", X_train.shape[1])\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0c373-c784-46ec-8325-b845d67c7ab8",
   "metadata": {},
   "source": [
    "## Task 4.2: Multinomial Naive Bayes [3 points]\n",
    "\n",
    "In this task you're asked to train a **Multinomial Naive Bayes** model on the reviews dataset. You're also asked to report the K-fold cross-validation. During the cross-validation the dataset gets split into K distinct subsets. At every iteration one of the subsets is removed from the training set, the model is trained on the rest and evaluated on the removed one. At the end the average of the K estimators is reported.\n",
    "\n",
    "**TODO:** Train *MultinomialNB* model from *sklearn.naive_bayes*. Report the 5-fold cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e07ed7-10d2-4ca7-8f6f-c42cea9ed4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# Transform reviews to the words-frequency matrix X.                  # \n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "cv_score = None\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bed03-31d1-4e61-9e4c-431bfa84d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross-validation accuracy score: {:.2f}\".format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650c237-59fe-4651-96ad-6d07a1022ed1",
   "metadata": {},
   "source": [
    "Let us now train the model on the whole training dataset, make the predictions and save them to the submission file.\n",
    "\n",
    "**TODO:** Train your model on the whole training set, make predictions and prepare a DataFrame for saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bce66-11c3-4b68-97cd-11789960a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# Train the model on X_train, y_train and make predictions for X_test.#\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "predictions = None\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9292b-0588-4ba4-8d95-47e03bde5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# Create a submission DataFrame that has a single column \"sentiment\"  #\n",
    "# With \"positive\"/\"negative\" values according to the predictions.     #\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "submission = None\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################\n",
    "\n",
    "submission[\"id\"] = np.arange(submission.shape[0])\n",
    "submission.to_csv(\"simple_baseline.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e292b-6a6d-45cb-a406-5dba083eddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5a16c-7098-4b19-9319-806b2c765dc6",
   "metadata": {},
   "source": [
    "## Task 4.3: Time for the Competition! [10 points + 15 bonus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19544c-2c85-45c3-a65b-0c36f81ea945",
   "metadata": {},
   "source": [
    "In this task you need to improve the accuracy of the model as much as you can. Here are several techniques that you can use:\n",
    "\n",
    "1) **Tune your hyper-parameters.** Try *GridSerachCV* function from *sklearn.model_selection* to find the best set of hyperparameters.\n",
    "\n",
    "2) **Feature engineering.** Play with the representation of the textual data. We only tried one, but there are more (e.g. TF-IDF Vectorizer is another powerful method to transform text to a vector, taking into account the rareness of the words across the texts). Also do not hesitate to play with the arguments of the *Vectorizers*. \n",
    "\n",
    "3) **Change your model.** You are not restricted to train *Naive Bayes* only. You can use whatever algorithm you're already familiar with. Moreover, you can use the algorithms that you get to know during these 3 weeks of solving this assignment. E.g. give *RandomForests* a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb87cbe-3d0f-4433-99af-537bb68a374a",
   "metadata": {},
   "source": [
    "When your model is trained, make the predictions for the test reviews and upload the file with those predictions to the [competition page](https://www.kaggle.com/t/a075874d324e402892eb5b4a31755fbb). You can find the formatting instructions to the submission file in the *Evaluation* section on the website.\n",
    "\n",
    "After your submission is uploaded it will automatically get scored and will appear in the leaderboard. Before the deadline all the submissions are scored on 30% of the test set (*Public leaderboard*), after the deadline the submissions are rescored on the remaining part of the test set (*Private leaderboard*). The Private leaderboard may differ from the Public one. Notice, that the **final** order is accroding to the **Private leadearboard**. This is done to prevent you from overfitting to the Public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11887e8e-e42a-4a2d-ac33-4069838dea3d",
   "metadata": {},
   "source": [
    "Notice that Kaggle limits you with **5** submissions per day, so make sure to test your model (e.g. with cross-validation) before you submit your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d44de8-791a-4c36-891c-c0b633f14a31",
   "metadata": {},
   "source": [
    "Make sure that the TAs can easily identify your submissions, i.e. don't use nicknames that cannot be linked to your name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbec4b-f229-4fb9-9b85-fbe654bbae80",
   "metadata": {},
   "source": [
    "### Scoring rules\n",
    "You get **3 points** if you beat the **simple baseline**, that is the classifier trained in task 4.2.\n",
    "\n",
    "You get another **7 points** if you beat the **hard baseline**. This is the score of the hidden classifier that is trained by TAs, you don't have access to its code.\n",
    "\n",
    "You get **5 bonus points** if you end up among **top-15** in the final leaderboard.\n",
    "\n",
    "You get another **5 bonus points** if you end up in **top-10** in the final leaderboard.\n",
    "\n",
    "And finally another **5 bonus points** will be given to those who end up in **top-5**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f7eec2cdae8bca0e892150c2fd69df9b979e330094e73ffaa909ee122a3ac16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
